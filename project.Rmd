---
title: "Practical Machine Learning - Project"
output: html_document
---


Data:
The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

This study collected data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the study website: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). If you use the document for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

The goal of this project is to predict the manner in which participants did the exercise. This is the "classe" variable in the training set.

## Building the model to predict activity quality from activity monitors.

The steps were
- Reading in the data from the csv files
```{r}
##read in the data files
pmltraining <- read.csv("pml-training.csv",na.strings=c("NA",""))
pmltesting <- read.csv("pml-testing.csv",na.strings=c("NA",""))


```
## Inspecting the data and choosing which columns to use as predictors: 
 1. Using the simplest approach any columns including one or more missing values were dropped. A more sophisticated approach would be to count the number of missing values and to apply a threshold.
```{r read_files}
## check columns have some data, empty strings were transformed to NA by read.csv
#sapply(pmltraining, function(x) sum(is.na(x)))
## rough and ready approach, remove columns that aren't complete
pmltraining2 <- pmltraining[,colSums(is.na(pmltraining)) == 0]
```
 
 2. When building models the first 7 columns were dropped because they do not contain activity measures. 
 
## Building and choosing a model
 1. First attempt was a simple tree model: but the accuracy was only 60% 
```{r simple_tree, cache=TRUE}
 ##load libraries
library(caret)

##define tune parameters to fit model with cross validation 
ctrl <- trainControl(method = "repeatedcv", repeats = 5)

##exclude first 7 columns that are not useful for prediction including row number and subject name
#first try a simple tree model
modfit <- train(pmltraining2$classe ~., method="rpart", trControl = ctrl, data=pmltraining2[,c(1,2,3,4,5,6,7)])
#use model to predict in sample results
confusionMatrix(pmltraining$classe,predict(modfit,pmltraining2))
```
 
2. The second attempt was a random forest model with cross validation: because of performance limitations, the model was build on a small random sample of the training data.
```{r random_forest, cache=TRUE}
##accuracy of the simple tree model was only 60%, so try a random forest model with a small, randomly selected set of observations
##define tune parameters to fit model with cross validation 
ctrl <- trainControl(method = "repeatedcv", repeats = 5)

inTrain <- createDataPartition(pmltraining2$classe, p=0.05)[[1]]
pmltraining2training <- pmltraining2[inTrain,]
modfitrf <- train(pmltraining2training$classe ~., method="rf", trControl = ctrl, data=pmltraining2training[,-c(1,2,3,4,5,6,7)])
```

## Cross validation and the expected out of sample error
The random forest model had a much better accuracy of 90% when cross validated on the full set of training data. 
 
The test data does not contain the outcome variable 'classe' so it was not possible to verify the out of sample error, but it can be expected to be a little worse than the in sample error.
```{r results}
##better accuracy than a simple tree model, 90% on in sample data 
confusionMatrix(pmltraining$classe,predict(modfitrf,pmltraining2[,-c(1,2)]))

##error rate
modfitrf$finalModel
```

## Using the prediction model to predict the 20 supplied test cases. 
The random forest model was used to predict the classe variable from the supplied test data.
```{r write_files}
##apply model to test data and write each result to a file for submission
answers <- predict(modfitrf,pmltesting)
#load a function to write out each answer to an individual file :
  
  pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
      filename = paste0("problem_id_",i,".txt")
      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
#write files as specified in the submission instructions, one per file ..
pml_write_files(answers)
```